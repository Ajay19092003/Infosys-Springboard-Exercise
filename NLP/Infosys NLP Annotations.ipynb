{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49f77085",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\AjayG\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\AjayG\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\AjayG\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping chunkers\\maxent_ne_chunker.zip.\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\AjayG\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\words.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Download necessary NLTK data (if not already downloaded)\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "\n",
    "# Load sample3.txt file\n",
    "file_path = 'C://Users//AjayG//Downloads//NLP Files//NLP Files//SampleText.txt'\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    sample_text = file.read()\n",
    "\n",
    "# Tokenize the sample text\n",
    "sample_tokens = nltk.word_tokenize(sample_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94b9038e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the sample text\n",
    "sample_tokens = nltk.word_tokenize(sample_text)\n",
    "\n",
    "# Perform POS tagging\n",
    "pos_tags = nltk.pos_tag(sample_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3cbba0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('NLP', 'ORGANIZATION'), ('NLP', 'ORGANIZATION'), ('NLP', 'ORGANIZATION'), ('LLMs', 'ORGANIZATION'), ('NLP', 'ORGANIZATION'), ('GPS', 'ORGANIZATION'), ('NLP', 'ORGANIZATION')]\n"
     ]
    }
   ],
   "source": [
    "# Perform Named Entity Recognition\n",
    "named_entities_tree = nltk.ne_chunk(pos_tags)\n",
    "\n",
    "# Extract named entities from the tree\n",
    "named_entities = []\n",
    "for subtree in named_entities_tree:\n",
    "    if isinstance(subtree, nltk.Tree):\n",
    "        entity_name = \" \".join([token for token, pos in subtree.leaves()])\n",
    "        entity_type = subtree.label()\n",
    "        named_entities.append((entity_name, entity_type))\n",
    "\n",
    "# Print the named entities\n",
    "print(named_entities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0abe4c12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('NLP', 'ORGANIZATION'), ('NLP', 'ORGANIZATION'), ('NLP', 'ORGANIZATION'), ('LLMs', 'ORGANIZATION'), ('NLP', 'ORGANIZATION'), ('GPS', 'ORGANIZATION'), ('NLP', 'ORGANIZATION')]\n"
     ]
    }
   ],
   "source": [
    "# Example dictionary of known entities (expand as needed)\n",
    "known_entities = {\n",
    "    'person': ['John Doe', 'Jane Smith', 'Alice Johnson'],\n",
    "    'organization': ['Google', 'Microsoft', 'Apple'],\n",
    "    'location': ['New York', 'California', 'Texas'],\n",
    "    'date': ['January 1, 2020', 'February 14, 2021', 'March 3, 2022'],\n",
    "    'time': ['10:00 AM', '2:00 PM', '5:30 PM'],\n",
    "    'GPE': ['USA', 'UK', 'Canada']\n",
    "}\n",
    "\n",
    "# Function to look up entities\n",
    "def lookup_entity(entity_name):\n",
    "    for entity_type, entity_list in known_entities.items():\n",
    "        if entity_name in entity_list:\n",
    "            return entity_type\n",
    "    return None\n",
    "\n",
    "# Cross-reference named entities with the known entities dictionary\n",
    "corrected_named_entities = []\n",
    "for entity_name, entity_type in named_entities:\n",
    "    correct_entity_type = lookup_entity(entity_name)\n",
    "    if correct_entity_type:\n",
    "        corrected_named_entities.append((entity_name, correct_entity_type))\n",
    "    else:\n",
    "        corrected_named_entities.append((entity_name, entity_type))\n",
    "\n",
    "# Print the corrected named entities\n",
    "print(corrected_named_entities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90391fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
